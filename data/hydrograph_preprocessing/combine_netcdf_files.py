#!/usr/bin/env python3
"""
Combine multiple NetCDF climatology files into a single dataset using Dask.

This script combines NetCDF files generated by process_streamflow_climatology.py
into a single merged dataset, separating historical and projection data to avoid
indexing conflicts, then merging them together.
"""

import sys
import argparse
import traceback
from pathlib import Path
from datetime import datetime
import xarray as xr
from dask.distributed import Client
import warnings

# Suppress some common warnings from xarray/dask
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Combine NetCDF climatology files using Dask",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "input_dir",
        type=str,
        help="Directory containing NetCDF files to combine"
    )
    
    parser.add_argument(
        "output_file",
        type=str,
        help="Path for output combined NetCDF file"
    )
    
    parser.add_argument(
        "--pattern",
        type=str,
        default="*_doy_mmm_by_era.nc",
        help="Glob pattern to match NetCDF files"
    )
    
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Number of Dask workers"
    )
    
    parser.add_argument(
        "--threads-per-worker",
        type=int,
        default=6,
        help="Number of threads per Dask worker"
    )
    
    return parser.parse_args()


def open_and_combine(file_paths, n_workers=4, threads_per_worker=6):
    """
    Open and combine a list of file paths into a single xarray dataset.
    
    Uses xarray's built-in combining functions for efficiency.
    
    Parameters:
    -----------
    file_paths : list of Path objects
        List of NetCDF file paths to combine
    n_workers : int
        Number of Dask workers
    threads_per_worker : int
        Number of threads per worker
        
    Returns:
    --------
    xarray.Dataset
        Combined dataset
    """
    
    print(f"Combining {len(file_paths)} files ... started at: {datetime.now().isoformat()}")
    
    # Use Dask distributed client with more reasonable memory limits
    try:
        with Client(n_workers=n_workers, threads_per_worker=threads_per_worker) as client:
            print(f"Dask client started: {client}")
            sys.stdout.flush()
            
            print(f"Opening all {len(file_paths)} files with xarray...")
            sys.stdout.flush()
            
            # Use xarray's open_mfdataset for efficient multi-file opening
            # This is much more efficient than manual merging
            try:
                combined_ds = xr.open_mfdataset(
                    file_paths,
                    combine='by_coords',  # Automatically combine by coordinate values
                    concat_dim=None,      # Let xarray figure out the dimensions
                    combine_attrs='drop_conflicts',  # Handle attribute conflicts
                    parallel=True,       # Use dask for parallel processing
                    engine='netcdf4'     # Use netcdf4 engine
                )
                
                print("Successfully opened and combined all files")
                sys.stdout.flush()
                
            except Exception as open_error:
                print(f"open_mfdataset failed, falling back to manual merge: {open_error}")
                sys.stderr.flush()
                
                # Fallback to simple merge approach
                datasets = []
                for i, file_path in enumerate(file_paths):
                    print(f"Opening file {i+1}/{len(file_paths)}: {file_path.name}...")
                    if i % 10 == 0:  # Progress update every 10 files
                        sys.stdout.flush()
                    ds = xr.open_dataset(file_path)
                    
                    # Fix string coordinates immediately for this file only
                    string_coords = ['model', 'scenario', 'landcover', 'era']
                    for coord_name in string_coords:
                        if coord_name in ds.coords:
                            ds[coord_name] = ds[coord_name].astype('U')  # Unicode strings
                    
                    datasets.append(ds)
                
                print("Merging datasets with simple merge...")
                sys.stdout.flush()
                combined_ds = xr.merge(datasets, combine_attrs="drop_conflicts")
                print("Simple merge completed")
                sys.stdout.flush()
                    
    except Exception as e:
        print(f"ERROR: Failed to initialize Dask client or process files: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        raise

    # String coordinates are already fixed during file opening, no need to fix them again
    print("String coordinates were fixed during individual file processing")
    sys.stdout.flush()

    print(f"Combining completed at: {datetime.now().isoformat()}")
    print(f"Final dataset dimensions: {combined_ds.dims}")
    
    return combined_ds


def main():
    """Main function to combine NetCDF files."""
    args = parse_arguments()
    
    # Convert to Path object
    input_dir = Path(args.input_dir)
    output_file = Path(args.output_file)
    
    # Validate input directory
    if not input_dir.exists():
        print(f"Error: Input directory does not exist: {input_dir}", file=sys.stderr)
        sys.exit(1)
    
    # Find NetCDF files
    nc_files = list(input_dir.glob(args.pattern))
    
    if not nc_files:
        print(f"No NetCDF files found matching pattern '{args.pattern}' in {input_dir}")
        sys.exit(1)
    
    print(f"Found {len(nc_files)} NetCDF files to combine")
    for file in sorted(nc_files):
        print(f"  {file.name}")
    sys.stdout.flush()
    
    # Create output directory if it doesn't exist
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        # Combine files
        print(f"Starting file combination with {args.workers} workers and {args.threads_per_worker} threads per worker")
        sys.stdout.flush()
        
        combined_ds = open_and_combine(
            nc_files, 
            args.workers, 
            args.threads_per_worker
        )
        
        # Add global attributes
        print("Adding global attributes...")
        sys.stdout.flush()
        combined_ds.attrs.update({
            'title': 'Combined Streamflow Daily Climatologies',
            'description': 'Daily climatology statistics (min, mean, max) by era and model',
            'created': datetime.now().isoformat()
        })
        
        # Print detailed dataset information before saving
        print("=== DATASET DIAGNOSTICS BEFORE SAVING ===")
        print(f"Dataset dimensions: {combined_ds.dims}")
        print(f"Dataset coordinates: {list(combined_ds.coords)}")
        print(f"Data variables: {list(combined_ds.data_vars)}")
        
        # Check for actual data in each variable
        for var_name in combined_ds.data_vars:
            var_data = combined_ds[var_name]
            print(f"{var_name}: shape={var_data.shape}, dtype={var_data.dtype}")
            
            # Count non-NaN values (load a small sample to check)
            try:
                sample = var_data.isel({dim: slice(0, min(10, var_data.sizes[dim])) 
                                      for dim in var_data.dims})
                sample_loaded = sample.load()
                finite_count = sample_loaded.count().values
                total_sample = sample_loaded.size
                print(f"  Sample check: {finite_count}/{total_sample} finite values")
            except Exception as e:
                print(f"  Could not check sample data: {e}")
        
        print(f"==========================================")
        sys.stdout.flush()
        
        # Save combined dataset with proper string encoding and chunking
        print(f"Saving combined dataset to: {output_file}")
        print(f"Final dataset dimensions: {combined_ds.dims}")
        
        # Print memory usage estimate before saving
        try:
            # Try to get actual size without loading everything
            total_bytes = 0
            for var_name in combined_ds.data_vars:
                var_size = combined_ds[var_name].nbytes
                total_bytes += var_size
                print(f"{var_name}: estimated {var_size / 1e9:.2f} GB")
            print(f"Total uncompressed dataset size estimate: {total_bytes / 1e9:.2f} GB")
        except Exception as size_error:
            print(f"Could not estimate dataset size: {size_error}")
        sys.stdout.flush()
        
        # Create encoding dict with compression for data variables and strings for coordinates
        encoding = {}
        
        # Add compression for all data variables
        for var_name in combined_ds.data_vars:
            encoding[var_name] = {
                'zlib': True, 
                'complevel': 4,  # Good balance of speed vs compression
                'shuffle': True  # Improves compression for scientific data
            }
        
        # Encoding for string coordinates - use Unicode to match our earlier fixing
        string_coords = ['model', 'scenario', 'landcover', 'era']
        for coord_name in string_coords:
            if coord_name in combined_ds.coords:
                encoding[coord_name] = {'dtype': 'U'}  # Use Unicode strings
        
        try:
            print("Starting to write NetCDF file (this may take some time for large datasets)...")
            sys.stdout.flush()
            
            combined_ds.to_netcdf(output_file, format='NETCDF4', encoding=encoding)
            print("NetCDF file saved successfully")
            
            # Check the actual file size on disk
            if output_file.exists():
                actual_size = output_file.stat().st_size
                print(f"Actual file size on disk: {actual_size / 1e9:.2f} GB ({actual_size} bytes)")
                
                # Quick verification - try to read the file back
                try:
                    test_ds = xr.open_dataset(output_file)
                    print(f"Verification - file dimensions: {test_ds.dims}")
                    print(f"Verification - data variables: {list(test_ds.data_vars)}")
                    
                    # Check if data is actually there
                    for var_name in test_ds.data_vars:
                        var_shape = test_ds[var_name].shape
                        var_size = test_ds[var_name].size
                        print(f"  {var_name}: shape={var_shape}, size={var_size}")
                    
                    test_ds.close()
                except Exception as verify_error:
                    print(f"WARNING: Could not verify saved file: {verify_error}", file=sys.stderr)
            else:
                print("ERROR: Output file was not created!", file=sys.stderr)
                
            sys.stdout.flush()
            
        except Exception as save_error:
            print(f"ERROR: Failed to save NetCDF file: {save_error}", file=sys.stderr)
            print(f"Full traceback:", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            sys.stderr.flush()
            raise
        
        # Clean up
        combined_ds.close()
        
        print(f"Successfully combined {len(nc_files)} files into {output_file}")
        sys.stdout.flush()
        
    except Exception as e:
        print(f"ERROR: Failed to combine files: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        sys.exit(1)


if __name__ == "__main__":
    main()

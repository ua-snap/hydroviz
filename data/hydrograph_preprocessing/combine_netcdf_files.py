#!/usr/bin/env python3
"""
Combine multiple NetCDF climatology files into a single dataset using Dask.

This script combines NetCDF files generated by process_streamflow_climatology.py
into a single merged dataset, separating historical and projection data to avoid
indexing conflicts, then merging them together.
"""

import sys
import argparse
import traceback
from pathlib import Path
from datetime import datetime
import xarray as xr
from dask.distributed import Client
import warnings

# Suppress some common warnings from xarray/dask
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Combine NetCDF climatology files using Dask",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "input_dir",
        type=str,
        help="Directory containing NetCDF files to combine"
    )
    
    parser.add_argument(
        "output_file",
        type=str,
        help="Path for output combined NetCDF file"
    )
    
    parser.add_argument(
        "--pattern",
        type=str,
        default="*_doy_mmm_by_era.nc",
        help="Glob pattern to match NetCDF files"
    )
    
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Number of Dask workers"
    )
    
    parser.add_argument(
        "--threads-per-worker",
        type=int,
        default=6,
        help="Number of threads per Dask worker"
    )
    
    return parser.parse_args()


def open_and_combine(file_paths, n_workers=4, threads_per_worker=6):
    """
    Open and combine a list of file paths into a single xarray dataset.
    
    To avoid indexing errors with the time dimension, we separate historical 
    and projected data, open them separately, and then combine them.
    
    Parameters:
    -----------
    file_paths : list of Path objects
        List of NetCDF file paths to combine
    n_workers : int
        Number of Dask workers
    threads_per_worker : int
        Number of threads per worker
        
    Returns:
    --------
    xarray.Dataset
        Combined dataset
    """
    
    print(f"Combining {len(file_paths)} files ... started at: {datetime.now().isoformat()}")
    
    # Use Dask distributed client
    try:
        with Client(n_workers=n_workers, threads_per_worker=threads_per_worker) as client:
            print(f"Dask client started: {client}")
            sys.stdout.flush()
            
            datasets = []
            
            # Open each file individually
            for i, file_path in enumerate(file_paths):
                try:
                    print(f"Opening file {i+1}/{len(file_paths)}: {file_path.name}...")
                    sys.stdout.flush()
                    
                    ds = xr.open_dataset(file_path)
                    ds = ds.load()
                    datasets.append(ds)
                    print(f"  Loaded successfully: dims: {ds.dims}")
                    sys.stdout.flush()
                    
                except Exception as e:
                    print(f"ERROR: Failed to open file {file_path}: {e}", file=sys.stderr)
                    print(f"Full traceback:", file=sys.stderr)
                    traceback.print_exc(file=sys.stderr)
                    sys.stderr.flush()
                    raise
                    
    except Exception as e:
        print(f"ERROR: Failed to initialize Dask client or process files: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        raise
    
    # Combine datasets by merging (not concatenating) to handle overlapping coordinates
    print("Merging datasets...")
    sys.stdout.flush()
    try:
        combined_ds = xr.merge(datasets, combine_attrs="drop_conflicts")
        print("Merge completed successfully")
        sys.stdout.flush()
    except Exception as e:
        print(f"ERROR: Failed to merge datasets: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        # Print dataset information for debugging
        print(f"Dataset information for debugging:", file=sys.stderr)
        for i, ds in enumerate(datasets):
            print(f"  Dataset {i+1}: coords={list(ds.coords)}, dims={ds.dims}", file=sys.stderr)
        sys.stderr.flush()
        sys.exit(1)

    # Fix string coordinate dtypes to prevent truncation
    try:
        string_coords = ['model', 'scenario', 'landcover', 'era']
        for coord_name in string_coords:
            if coord_name in combined_ds.coords:
                # Convert to object dtype to allow variable-length strings
                coord_values = combined_ds[coord_name].values
                # Ensure all values are strings and find max length
                str_values = [str(val) for val in coord_values]
                combined_ds = combined_ds.assign_coords({coord_name: str_values})
                print(f"Fixed {coord_name} coordinate: {combined_ds[coord_name].values}")
                sys.stdout.flush()
    except Exception as e:
        print(f"ERROR: Failed to fix string coordinate dtypes: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        sys.exit(1)

    print(f"Combining completed at: {datetime.now().isoformat()}")
    print(f"Final dataset dimensions: {combined_ds.dims}")
    
    return combined_ds


def main():
    """Main function to combine NetCDF files."""
    args = parse_arguments()
    
    # Convert to Path object
    input_dir = Path(args.input_dir)
    output_file = Path(args.output_file)
    
    # Validate input directory
    if not input_dir.exists():
        print(f"Error: Input directory does not exist: {input_dir}", file=sys.stderr)
        sys.exit(1)
    
    # Find NetCDF files
    nc_files = list(input_dir.glob(args.pattern))
    
    if not nc_files:
        print(f"No NetCDF files found matching pattern '{args.pattern}' in {input_dir}")
        sys.exit(1)
    
    print(f"Found {len(nc_files)} NetCDF files to combine")
    for file in sorted(nc_files):
        print(f"  {file.name}")
    sys.stdout.flush()
    
    # Create output directory if it doesn't exist
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        # Combine files
        print(f"Starting file combination with {args.workers} workers and {args.threads_per_worker} threads per worker")
        sys.stdout.flush()
        
        combined_ds = open_and_combine(
            nc_files, 
            args.workers, 
            args.threads_per_worker
        )
        
        # Add global attributes
        print("Adding global attributes...")
        sys.stdout.flush()
        combined_ds.attrs.update({
            'title': 'Combined Streamflow Daily Climatologies',
            'description': 'Daily climatology statistics (min, mean, max) by era and model',
            'created': datetime.now().isoformat(),
            'source_files': [f.name for f in nc_files]
        })
        
        # Save combined dataset with proper string encoding
        print(f"Saving combined dataset to: {output_file}")
        sys.stdout.flush()
        
        # Create encoding dict to ensure string coordinates use variable-length strings
        encoding = {}
        string_coords = ['model', 'scenario', 'landcover', 'era']
        for coord_name in string_coords:
            if coord_name in combined_ds.coords:
                encoding[coord_name] = {'dtype': 'S1'}  # Use variable-length strings
        
        try:
            combined_ds.to_netcdf(output_file, format='NETCDF4', encoding=encoding)
            print("NetCDF file saved successfully")
            sys.stdout.flush()
        except Exception as save_error:
            print(f"ERROR: Failed to save NetCDF file: {save_error}", file=sys.stderr)
            print(f"Full traceback:", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            sys.stderr.flush()
            raise
        
        # Clean up
        combined_ds.close()
        
        print(f"Successfully combined {len(nc_files)} files into {output_file}")
        sys.stdout.flush()
        
    except Exception as e:
        print(f"ERROR: Failed to combine files: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        sys.exit(1)


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
Combine multiple NetCDF climatology files into a single dataset using Dask.

This script combines NetCDF files generated by process_streamflow_climatology.py
into a single merged dataset, separating historical and projection data to avoid
indexing conflicts, then merging them together.
"""

import sys
import argparse
import traceback
from pathlib import Path
from datetime import datetime
import xarray as xr
from dask.distributed import Client
import warnings

# Suppress some common warnings from xarray/dask
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Combine NetCDF climatology files using Dask",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "input_dir",
        type=str,
        help="Directory containing NetCDF files to combine"
    )
    
    parser.add_argument(
        "output_file",
        type=str,
        help="Path for output combined NetCDF file"
    )
    
    parser.add_argument(
        "--pattern",
        type=str,
        default="*_doy_mmm_by_era.nc",
        help="Glob pattern to match NetCDF files"
    )
    
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Number of Dask workers"
    )
    
    parser.add_argument(
        "--threads-per-worker",
        type=int,
        default=6,
        help="Number of threads per Dask worker"
    )
    
    return parser.parse_args()


def open_and_combine(file_paths, n_workers=4, threads_per_worker=6):
    """
    Open and combine a list of file paths into a single xarray dataset.
    
    Uses chunked operations and avoids loading all data into memory
    to handle large datasets efficiently.
    
    Parameters:
    -----------
    file_paths : list of Path objects
        List of NetCDF file paths to combine
    n_workers : int
        Number of Dask workers
    threads_per_worker : int
        Number of threads per worker
        
    Returns:
    --------
    xarray.Dataset
        Combined dataset
    """
    
    print(f"Combining {len(file_paths)} files ... started at: {datetime.now().isoformat()}")
    
    # Use Dask distributed client
    try:
        with Client(n_workers=n_workers, threads_per_worker=threads_per_worker, 
                   memory_limit='16GB') as client:
            print(f"Dask client started: {client}")
            sys.stdout.flush()
            
            # Define optimal chunk sizes for memory efficiency
            # Target chunks of ~100MB each
            chunk_sizes = {
                'doy': 50,  # Chunk day-of-year dimension
                'stat': -1,  # Keep stat dimension intact (small)
                'model': 1,  # Process one model at a time
                'scenario': 1,  # Process one scenario at a time
                'era': -1,  # Keep era dimension intact (small)
                'stream_id': 1000  # Chunk station dimension
            }
            
            datasets = []
            
            # Open each file individually with chunking
            for i, file_path in enumerate(file_paths):
                try:
                    print(f"Opening file {i+1}/{len(file_paths)}: {file_path.name}...")
                    sys.stdout.flush()
                    
                    # Open with chunking, don't load into memory
                    ds = xr.open_dataset(file_path, chunks=chunk_sizes)
                    
                    # Print dataset info without loading data
                    print(f"  Opened successfully: dims: {ds.dims}")
                    print(f"  Data variables: {list(ds.data_vars)}")
                    print(f"  Coordinates: {list(ds.coords)}")
                    
                    # Check memory usage estimate
                    total_size = sum(var.nbytes for var in ds.data_vars.values())
                    print(f"  Estimated size: {total_size / 1e9:.2f} GB")
                    sys.stdout.flush()
                    
                    datasets.append(ds)
                    
                except Exception as e:
                    print(f"ERROR: Failed to open file {file_path}: {e}", file=sys.stderr)
                    print(f"Full traceback:", file=sys.stderr)
                    traceback.print_exc(file=sys.stderr)
                    sys.stderr.flush()
                    raise
                    
    except Exception as e:
        print(f"ERROR: Failed to initialize Dask client or process files: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        raise
    
    # Combine datasets incrementally to avoid memory issues
    print("Combining datasets incrementally...")
    sys.stdout.flush()
    try:
        # Start with the first dataset
        combined_ds = datasets[0]
        print(f"Starting with dataset 1: {datasets[0].dims}")
        
        # Merge datasets one by one to avoid large memory allocations
        for i, ds in enumerate(datasets[1:], 2):
            print(f"Merging dataset {i}/{len(datasets)}...")
            sys.stdout.flush()
            
            try:
                # Use merge with chunked data - this keeps data on disk
                combined_ds = xr.merge([combined_ds, ds], 
                                     combine_attrs="drop_conflicts",
                                     compat='override')
                print(f"  Successfully merged dataset {i}")
                sys.stdout.flush()
                
            except Exception as merge_error:
                print(f"ERROR: Failed to merge dataset {i}: {merge_error}", file=sys.stderr)
                print(f"Dataset {i} info: coords={list(ds.coords)}, dims={ds.dims}", file=sys.stderr)
                print(f"Combined so far: coords={list(combined_ds.coords)}, dims={combined_ds.dims}", file=sys.stderr)
                sys.stderr.flush()
                raise
        
        print("Incremental merge completed successfully")
        sys.stdout.flush()
        
    except Exception as e:
        print(f"ERROR: Failed to merge datasets: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        # Print dataset information for debugging
        print(f"Dataset information for debugging:", file=sys.stderr)
        for i, ds in enumerate(datasets):
            print(f"  Dataset {i+1}: coords={list(ds.coords)}, dims={ds.dims}", file=sys.stderr)
        sys.stderr.flush()
        sys.exit(1)

    # Fix string coordinate dtypes to prevent truncation
    try:
        string_coords = ['model', 'scenario', 'landcover', 'era']
        for coord_name in string_coords:
            if coord_name in combined_ds.coords:
                # Convert to object dtype to allow variable-length strings
                coord_values = combined_ds[coord_name].values
                # Ensure all values are strings and find max length
                str_values = [str(val) for val in coord_values]
                combined_ds = combined_ds.assign_coords({coord_name: str_values})
                print(f"Fixed {coord_name} coordinate: {combined_ds[coord_name].values}")
                sys.stdout.flush()
    except Exception as e:
        print(f"ERROR: Failed to fix string coordinate dtypes: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        sys.exit(1)

    print(f"Combining completed at: {datetime.now().isoformat()}")
    print(f"Final dataset dimensions: {combined_ds.dims}")
    
    return combined_ds


def main():
    """Main function to combine NetCDF files."""
    args = parse_arguments()
    
    # Convert to Path object
    input_dir = Path(args.input_dir)
    output_file = Path(args.output_file)
    
    # Validate input directory
    if not input_dir.exists():
        print(f"Error: Input directory does not exist: {input_dir}", file=sys.stderr)
        sys.exit(1)
    
    # Find NetCDF files
    nc_files = list(input_dir.glob(args.pattern))
    
    if not nc_files:
        print(f"No NetCDF files found matching pattern '{args.pattern}' in {input_dir}")
        sys.exit(1)
    
    print(f"Found {len(nc_files)} NetCDF files to combine")
    for file in sorted(nc_files):
        print(f"  {file.name}")
    sys.stdout.flush()
    
    # Create output directory if it doesn't exist
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        # Combine files
        print(f"Starting file combination with {args.workers} workers and {args.threads_per_worker} threads per worker")
        sys.stdout.flush()
        
        combined_ds = open_and_combine(
            nc_files, 
            args.workers, 
            args.threads_per_worker
        )
        
        # Add global attributes
        print("Adding global attributes...")
        sys.stdout.flush()
        combined_ds.attrs.update({
            'title': 'Combined Streamflow Daily Climatologies',
            'description': 'Daily climatology statistics (min, mean, max) by era and model',
            'created': datetime.now().isoformat()
        })
        
        # Save combined dataset with proper string encoding and chunking
        print(f"Saving combined dataset to: {output_file}")
        print(f"Final dataset dimensions: {combined_ds.dims}")
        
        # Print memory usage estimate before saving
        try:
            total_size = sum(var.nbytes for var in combined_ds.data_vars.values())
            print(f"Total dataset size estimate: {total_size / 1e9:.2f} GB")
        except:
            print("Could not estimate dataset size")
        sys.stdout.flush()
        
        # Create encoding dict for string coordinates only (no compression)
        encoding = {}
        string_coords = ['model', 'scenario', 'landcover', 'era']
        
        # Encoding for string coordinates only
        for coord_name in string_coords:
            if coord_name in combined_ds.coords:
                encoding[coord_name] = {'dtype': 'S1'}  # Use variable-length strings
        
        try:
            print("Starting to write NetCDF file (this may take some time for large datasets)...")
            sys.stdout.flush()
            
            combined_ds.to_netcdf(output_file, format='NETCDF4', encoding=encoding)
            print("NetCDF file saved successfully")
            sys.stdout.flush()
            
        except Exception as save_error:
            print(f"ERROR: Failed to save NetCDF file: {save_error}", file=sys.stderr)
            print(f"Full traceback:", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            sys.stderr.flush()
            raise
        
        # Clean up
        combined_ds.close()
        
        print(f"Successfully combined {len(nc_files)} files into {output_file}")
        sys.stdout.flush()
        
    except Exception as e:
        print(f"ERROR: Failed to combine files: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        sys.exit(1)


if __name__ == "__main__":
    main()

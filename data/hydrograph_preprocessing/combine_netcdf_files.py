#!/usr/bin/env python3
"""
Combine multiple NetCDF climatology files into a single dataset using Dask.

This script combines NetCDF files generated by process_streamflow_climatology.py
into a single merged dataset, separating historical and projection data to avoid
indexing conflicts, then merging them together.
"""

import sys
import argparse
from pathlib import Path
from datetime import datetime
import xarray as xr
from dask.distributed import Client
import warnings

# Suppress some common warnings from xarray/dask
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Combine NetCDF climatology files using Dask",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "input_dir",
        type=str,
        help="Directory containing NetCDF files to combine"
    )
    
    parser.add_argument(
        "output_file",
        type=str,
        help="Path for output combined NetCDF file"
    )
    
    parser.add_argument(
        "--pattern",
        type=str,
        default="*_doy_mmm_by_era.nc",
        help="Glob pattern to match NetCDF files"
    )
    
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Number of Dask workers"
    )
    
    parser.add_argument(
        "--threads-per-worker",
        type=int,
        default=6,
        help="Number of threads per Dask worker"
    )
    
    return parser.parse_args()


def open_and_combine(file_paths, n_workers=4, threads_per_worker=6):
    """
    Open and combine a list of file paths into a single xarray dataset.
    
    To avoid indexing errors with the time dimension, we separate historical 
    and projected data, open them separately, and then combine them.
    
    Parameters:
    -----------
    file_paths : list of Path objects
        List of NetCDF file paths to combine
    n_workers : int
        Number of Dask workers
    threads_per_worker : int
        Number of threads per worker
        
    Returns:
    --------
    xarray.Dataset
        Combined dataset
    """
    
    print(f"Combining {len(file_paths)} files ... started at: {datetime.now().isoformat()}")
    
    # Use Dask distributed client
    with Client(n_workers=n_workers, threads_per_worker=threads_per_worker) as client:
        
        datasets = []
        
        # Open each file individually
        for file_path in file_paths:
            print(f"Opening {file_path.name}...")
            ds = xr.open_dataset(file_path)
            ds = ds.load()
            datasets.append(ds)
            print(f"  Loaded: dims: {ds.dims}")
    
    # Combine datasets by merging (not concatenating) to handle overlapping coordinates
    print("Merging datasets...")
    combined_ds = xr.merge(datasets, combine_attrs="drop_conflicts")
    
    # Fix string coordinate dtypes to prevent truncation
    string_coords = ['model', 'scenario', 'landcover', 'era']
    for coord_name in string_coords:
        if coord_name in combined_ds.coords:
            # Convert to object dtype to allow variable-length strings
            coord_values = combined_ds[coord_name].values
            # Ensure all values are strings and find max length
            str_values = [str(val) for val in coord_values]
            combined_ds = combined_ds.assign_coords({coord_name: str_values})
            print(f"Fixed {coord_name} coordinate: {combined_ds[coord_name].values}")
    
    print(f"Combining completed at: {datetime.now().isoformat()}")
    print(f"Final dataset dimensions: {combined_ds.dims}")
    
    return combined_ds


def main():
    """Main function to combine NetCDF files."""
    args = parse_arguments()
    
    # Convert to Path object
    input_dir = Path(args.input_dir)
    output_file = Path(args.output_file)
    
    # Validate input directory
    if not input_dir.exists():
        print(f"Error: Input directory does not exist: {input_dir}", file=sys.stderr)
        sys.exit(1)
    
    # Find NetCDF files
    nc_files = list(input_dir.glob(args.pattern))
    
    if not nc_files:
        print(f"No NetCDF files found matching pattern '{args.pattern}' in {input_dir}")
        sys.exit(1)
    
    print(f"Found {len(nc_files)} NetCDF files to combine")
    for file in sorted(nc_files):
        print(f"  {file.name}")
    
    # Create output directory if it doesn't exist
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        # Combine files
        combined_ds = open_and_combine(
            nc_files, 
            args.workers, 
            args.threads_per_worker
        )
        
        # Add global attributes
        combined_ds.attrs.update({
            'title': 'Combined Streamflow Daily Climatologies',
            'description': 'Daily climatology statistics (min, mean, max) by era and model',
            'created': datetime.now().isoformat(),
            'source_files': [f.name for f in nc_files]
        })
        
        # Save combined dataset with proper string encoding
        print(f"Saving combined dataset to: {output_file}")
        
        # Create encoding dict to ensure string coordinates use variable-length strings
        encoding = {}
        string_coords = ['model', 'scenario', 'landcover', 'era']
        for coord_name in string_coords:
            if coord_name in combined_ds.coords:
                encoding[coord_name] = {'dtype': 'S1'}  # Use variable-length strings
        
        combined_ds.to_netcdf(output_file, format='NETCDF4', encoding=encoding)
        
        # Clean up
        combined_ds.close()
        
        print(f"Successfully combined {len(nc_files)} files into {output_file}")
        
    except Exception as e:
        print(f"Error combining files: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
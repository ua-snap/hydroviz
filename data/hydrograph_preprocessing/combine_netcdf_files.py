#!/usr/bin/env python3
"""
Combine multiple NetCDF climatology files into a single dataset using Dask.

This script combines NetCDF files generated by process_streamflow_climatology.py
into a single merged dataset, separating historical and projection data to avoid
indexing conflicts, then merging them together.
"""

import sys
import argparse
import traceback
from pathlib import Path
from datetime import datetime
import xarray as xr
from dask.distributed import Client
from dask.callbacks import Callback
import threading
import time
import warnings
import psutil
import os

# Suppress some common warnings from xarray/dask
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def get_system_resources():
    """Get current system resource usage."""
    try:
        # Memory info
        memory = psutil.virtual_memory()
        mem_used_gb = memory.used / (1024**3)
        
        # Try to get SLURM memory limit from environment
        slurm_mem_limit = os.environ.get('SLURM_MEM_LIMIT', '750G')
        # Simple parsing - just strip any letters and assume GB
        mem_limit_gb = float(''.join(c for c in slurm_mem_limit if c.isdigit() or c == '.'))
        
        mem_percent = (mem_used_gb / mem_limit_gb) * 100
        
        # CPU info
        cpu_percent = psutil.cpu_percent(interval=0.1)
        load_avg = os.getloadavg()[0]  # 1-minute load average
        
        # I/O stats (if available)
        try:
            io_stats = psutil.disk_io_counters()
            io_wait = psutil.cpu_times_percent(interval=0.1).iowait if hasattr(psutil.cpu_times_percent(interval=0.1), 'iowait') else 0
        except:
            io_wait = 0
            
        return {
            'mem_used_gb': mem_used_gb,
            'mem_limit_gb': mem_limit_gb, 
            'mem_percent': mem_percent,
            'cpu_percent': cpu_percent,
            'load_avg': load_avg,
            'io_wait': io_wait
        }
    except Exception as e:
        # Fallback if psutil fails
        return {
            'mem_used_gb': 0,
            'mem_limit_gb': 750.0,
            'mem_percent': 0,
            'cpu_percent': 0,
            'load_avg': 0,
            'io_wait': 0
        }


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Combine NetCDF climatology files using Dask",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "input_dir",
        type=str,
        help="Directory containing NetCDF files to combine"
    )
    
    parser.add_argument(
        "output_file",
        type=str,
        help="Path for output combined NetCDF file"
    )
    
    parser.add_argument(
        "--pattern",
        type=str,
        default="*_doy_mmm_by_era.nc",
        help="Glob pattern to match NetCDF files"
    )
    
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Number of Dask workers"
    )
    
    parser.add_argument(
        "--threads-per-worker",
        type=int,
        default=6,
        help="Number of threads per Dask worker"
    )
    
    return parser.parse_args()


def open_and_combine(file_paths, n_workers=4, threads_per_worker=6):
    """
    Open and combine a list of file paths into a single xarray dataset.
    
    Uses xarray's built-in combining functions for efficiency.
    
    Parameters:
    -----------
    file_paths : list of Path objects
        List of NetCDF file paths to combine
    n_workers : int
        Number of Dask workers
    threads_per_worker : int
        Number of threads per worker
        
    Returns:
    --------
    xarray.Dataset
        Combined dataset
    """
    
    print(f"Combining {len(file_paths)} files ... started at: {datetime.now().isoformat()}")
    
    try:
        with Client(n_workers=n_workers, threads_per_worker=threads_per_worker) as client:
            print(f"Dask client started: {client}")
            sys.stdout.flush()
            
            print(f"Opening all {len(file_paths)} files with xarray...")
            sys.stdout.flush()
            
            # Use xarray's open_mfdataset for efficient multi-file opening
            # This is much more efficient than manual merging
            try:
                combined_ds = xr.open_mfdataset(
                    file_paths,
                    combine='by_coords',  # automatically combine by coordinate values
                    concat_dim=None,      # let xarray figure out the dimensions
                    combine_attrs='drop_conflicts',
                    parallel=True,
                    engine='netcdf4'
                )
                
                print("Successfully opened and combined all files")
                sys.stdout.flush()
                
            except Exception as open_error:
                print(f"open_mfdataset failed, falling back to manual merge: {open_error}")
                sys.stderr.flush()
                
                # Fallback to simple merge approach if open_mfdataset fails
                datasets = []
                for i, file_path in enumerate(file_paths):
                    print(f"Opening file {i+1}/{len(file_paths)}: {file_path.name}...")
                    if i % 10 == 0:  # Progress update every 10 files
                        sys.stdout.flush()
                    ds = xr.open_dataset(file_path)
                    
                    # Fix string coordinates immediately on opening
                    # xr.open_mfdataset should handle this, but we need it here on manual merge to avoid string truncation
                    string_coords = ['model', 'scenario', 'landcover', 'era']
                    for coord_name in string_coords:
                        if coord_name in ds.coords:
                            ds[coord_name] = ds[coord_name].astype('U')  # Unicode strings
                    
                    datasets.append(ds)
                
                print("Merging datasets with simple merge...")
                sys.stdout.flush()
                combined_ds = xr.merge(datasets, combine_attrs="drop_conflicts")
                print("Simple merge completed")
                sys.stdout.flush()
                    
    except Exception as e:
        print(f"ERROR: Failed to initialize Dask client or process files: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        raise

    print(f"Combining completed at: {datetime.now().isoformat()}")
    print(f"Final dataset dimensions: {combined_ds.dims}")
    
    return combined_ds


def main():
    """Main function to combine NetCDF files."""
    args = parse_arguments()
    
    # Convert to Path object
    input_dir = Path(args.input_dir)
    output_file = Path(args.output_file)
    
    # Validate input directory
    if not input_dir.exists():
        print(f"Error: Input directory does not exist: {input_dir}", file=sys.stderr)
        sys.exit(1)
    
    # Find NetCDF files
    nc_files = list(input_dir.glob(args.pattern))
    
    if not nc_files:
        print(f"No NetCDF files found matching pattern '{args.pattern}' in {input_dir}")
        sys.exit(1)
    
    print(f"Found {len(nc_files)} NetCDF files to combine")
    for file in sorted(nc_files):
        print(f"  {file.name}")
    sys.stdout.flush()
    
    # Create output directory if it doesn't exist
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        # Combine files
        print(f"Starting file combination with {args.workers} workers and {args.threads_per_worker} threads per worker")
        sys.stdout.flush()
        
        combined_ds = open_and_combine(
            nc_files, 
            args.workers, 
            args.threads_per_worker
        )
        
        # Add global attributes
        print("Adding global attributes...")
        sys.stdout.flush()
        combined_ds.attrs.update({
            'title': 'Combined Streamflow Daily Climatologies',
            'description': 'Daily climatology statistics (min, mean, max) by era and model',
            'created': datetime.now().isoformat()
        })
        
        # Print detailed dataset information before saving
        print("=== DATASET DIAGNOSTICS BEFORE SAVING ===")
        print(f"Dataset dimensions: {combined_ds.dims}")
        print(f"Dataset coordinates: {list(combined_ds.coords)}")
        print(f"Data variables: {list(combined_ds.data_vars)}")
        
        # Check for actual data in each variable
        for var_name in combined_ds.data_vars:
            var_data = combined_ds[var_name]
            print(f"{var_name}: shape={var_data.shape}, dtype={var_data.dtype}")
            
            # Count non-NaN values (load a small sample to check)
            try:
                sample = var_data.isel({dim: slice(0, min(10, var_data.sizes[dim])) 
                                      for dim in var_data.dims})
                sample_loaded = sample.load()
                finite_count = sample_loaded.count().values
                total_sample = sample_loaded.size
                print(f"  Sample check: {finite_count}/{total_sample} finite values")
            except Exception as e:
                print(f"  Could not check sample data: {e}")
        
        print(f"==========================================")
        sys.stdout.flush()
        
        # Save combined dataset with proper string encoding and chunking
        print(f"Saving combined dataset to: {output_file}")
        print(f"Final dataset dimensions: {combined_ds.dims}")
        
        # Print memory usage estimate before saving
        try:
            # Try to get actual size without loading everything
            total_bytes = 0
            for var_name in combined_ds.data_vars:
                var_size = combined_ds[var_name].nbytes
                total_bytes += var_size
                print(f"{var_name}: estimated {var_size / 1e9:.2f} GB")
            print(f"Total uncompressed dataset size estimate: {total_bytes / 1e9:.2f} GB")
        except Exception as size_error:
            print(f"Could not estimate dataset size: {size_error}")
        sys.stdout.flush()
        
        # Create encoding dict with compression for data variables and strings for coordinates
        encoding = {}
        
        # Add compression for all data variables
        for var_name in combined_ds.data_vars:
            encoding[var_name] = {
                'zlib': True, 
                'complevel': 4,  # Good balance of speed vs compression
                'shuffle': True  # Improves compression for scientific data
            }
        
        # Encoding for string coordinates - use Unicode to match our earlier fixing
        string_coords = ['model', 'scenario', 'landcover', 'era']
        for coord_name in string_coords:
            if coord_name in combined_ds.coords:
                encoding[coord_name] = {'dtype': 'U'}  # Use Unicode strings
        
        try:
            print("Starting to write NetCDF file (this may take some time for large datasets)...")
            print("Progress updates will appear below...")
            sys.stdout.flush()
                        
            # Progress tracking variables
            start_time = time.time()
            progress_info = {'completed': 0, 'total': 0, 'start_time': start_time}
            
            class ProgressCallback(Callback):
                def _start(self, dsk):
                    progress_info['total'] = len(dsk)
                    progress_info['completed'] = 0
                    progress_info['start_time'] = time.time()
                    print(f"Starting computation with {progress_info['total']} tasks...")
                    sys.stdout.flush()
                    
                def _finish(self, dsk, state, errored):
                    elapsed = time.time() - progress_info['start_time']
                    print(f"âœ“ Computation completed: {progress_info['total']}/{progress_info['total']} tasks in {elapsed/60:.1f} minutes")
                    sys.stdout.flush()
                    
                def _posttask(self, key, result, dsk, state, id):
                    progress_info['completed'] += 1
                    
                    # Print update every 50 tasks with timing and resource info
                    if progress_info['completed'] % 50 == 0 or progress_info['completed'] == progress_info['total']:
                        elapsed = time.time() - progress_info['start_time']
                        percent = (progress_info['completed'] / progress_info['total']) * 100
                        
                        # Get system resources
                        resources = get_system_resources()
                        
                        # Check output file size if it exists
                        file_size_gb = 0
                        try:
                            if output_file.exists():
                                file_size_gb = output_file.stat().st_size / (1024**3)
                        except:
                            pass
                        
                        print(f"Writing progress: {progress_info['completed']}/{progress_info['total']} tasks ({percent:.1f}%) | "
                              f"Elapsed: {elapsed/60:.1f}min")
                        print(f"  Resources: RAM {resources['mem_used_gb']:.1f}/{resources['mem_limit_gb']:.1f}GB ({resources['mem_percent']:.1f}%) | "
                              f"CPU {resources['cpu_percent']:.1f}% | Load {resources['load_avg']:.1f} | "
                              f"IOWait {resources['io_wait']:.1f}% | File {file_size_gb:.1f}GB")
                        sys.stdout.flush()
            
            # Write with progress monitoring
            with ProgressCallback():
                combined_ds.to_netcdf(output_file, format='NETCDF4', encoding=encoding)
                
            print("NetCDF file saved successfully")
            
            # Check the actual file size on disk
            if output_file.exists():
                actual_size = output_file.stat().st_size
                print(f"Actual file size on disk: {actual_size / 1e9:.2f} GB ({actual_size} bytes)")
                
                # Quick verification - try to read the file back
                try:
                    test_ds = xr.open_dataset(output_file)
                    print(f"Verification - file dimensions: {test_ds.dims}")
                    print(f"Verification - data variables: {list(test_ds.data_vars)}")
                    
                    # Check if data is actually there
                    for var_name in test_ds.data_vars:
                        var_shape = test_ds[var_name].shape
                        var_size = test_ds[var_name].size
                        print(f"  {var_name}: shape={var_shape}, size={var_size}")
                    
                    test_ds.close()
                except Exception as verify_error:
                    print(f"WARNING: Could not verify saved file: {verify_error}", file=sys.stderr)
            else:
                print("ERROR: Output file was not created!", file=sys.stderr)
                
            sys.stdout.flush()
            
        except Exception as save_error:
            print(f"ERROR: Failed to save NetCDF file: {save_error}", file=sys.stderr)
            print(f"Full traceback:", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            sys.stderr.flush()
            raise
        
        # Clean up
        combined_ds.close()
        
        print(f"Successfully combined {len(nc_files)} files into {output_file}")
        sys.stdout.flush()
        
    except Exception as e:
        print(f"ERROR: Failed to combine files: {e}", file=sys.stderr)
        print(f"Full traceback:", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        sys.exit(1)


if __name__ == "__main__":
    main()

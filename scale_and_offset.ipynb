{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scale_and_offset(min_value, max_value, n):\n",
    "    \"\"\"Function to compute the scale factor and add offset for packing data.\n",
    "    Args:\n",
    "        min_value (float): minimum value in the dataset\n",
    "        max_value (float): maximum value in the dataset\n",
    "        n (int): number of bits to use for packing\n",
    "    Returns:\n",
    "        tuple: scale factor and add offset\"\"\"\n",
    "    # stretch/compress data to the available packed range\n",
    "    scale_factor = (max_value - min_value) / (2**n - 1)\n",
    "    # translate the range to be symmetric about zero\n",
    "    add_offset = min_value + 2 ** (n - 1) * scale_factor\n",
    "    return (scale_factor, add_offset)\n",
    "\n",
    "\n",
    "def pack_dataset_by_var(ds, ds_packed, n):\n",
    "    \"\"\"Function to pack data in a dataset by variable, reducing the number of bits used to store the data.\n",
    "    Args:\n",
    "        ds (xarray.Dataset): input dataset\n",
    "        ds_packed (xarray.Dataset): output dataset with packed values. This should be a copy of the original dataset.\n",
    "        n (int): number of bits to use for packing\n",
    "    Returns:\n",
    "        xarray.Dataset: dataset with packed values\"\"\"\n",
    "    dim_names = list(ds.dims)\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        min_value = xr.DataArray.min(ds[var], skipna=True).values\n",
    "        max_value = xr.DataArray.max(ds[var], skipna=True).values\n",
    "        scale_factor, add_offset = compute_scale_and_offset(min_value, max_value, n)\n",
    "        packed_array = (ds[var].values - add_offset) / scale_factor\n",
    "        # apply floor function to all values in the packed array, while replacing NaNs with -9999\n",
    "        # convert to integer type using the number of bits specified by n\n",
    "        packed_array = np.floor(np.nan_to_num(packed_array, nan=-9999)).astype(\n",
    "            f\"int{n}\"\n",
    "        )\n",
    "\n",
    "        # overwrite the values in copied dataset with the packed values\n",
    "        ds_packed[var] = xr.DataArray(packed_array, dims=dim_names)\n",
    "\n",
    "        ds_packed[var].attrs = {\n",
    "            \"_FillValue\": -9999,\n",
    "            \"scale_factor\": scale_factor,\n",
    "            \"add_offset\": add_offset,\n",
    "        }\n",
    "\n",
    "    return ds_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"/beegfs/CMIP6/jdpaul3/hydroviz_data/nc/seg.nc\")\n",
    "ds_packed = ds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished packing 32-bit dataset\n",
      "Finished packing 16-bit dataset\n",
      "Finished packing 8-bit dataset\n"
     ]
    }
   ],
   "source": [
    "bits = [32, 16, 8]\n",
    "files = [\"/beegfs/CMIP6/jdpaul3/hydroviz_data/nc/seg.nc\"]  # seed with original file\n",
    "for n in bits:\n",
    "    ds_packed = pack_dataset_by_var(ds, ds_packed, n)\n",
    "    filename = f\"/beegfs/CMIP6/jdpaul3/scratch/seg_packed_{n}bit.nc\"\n",
    "    ds_packed.to_netcdf(filename)\n",
    "    print(f\"Finished packing {n}-bit dataset\")\n",
    "    files.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/beegfs/CMIP6/jdpaul3/hydroviz_data/nc/seg.nc  size:  5.031551872380078 GB\n",
      "9.8125\n",
      "nan\n",
      "/beegfs/CMIP6/jdpaul3/scratch/seg_packed_32bit.nc  size:  2.516008894890547 GB\n",
      "9.812499981097062\n",
      "nan\n",
      "/beegfs/CMIP6/jdpaul3/scratch/seg_packed_16bit.nc  size:  1.2582361213862896 GB\n",
      "9.811215381094073\n",
      "nan\n",
      "/beegfs/CMIP6/jdpaul3/scratch/seg_packed_8bit.nc  size:  0.629349734634161 GB\n",
      "9.529411764705884\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "def test_unpacked_values(ds):\n",
    "    # check two values we know are float & nan in the original dataset.... how do the packed & unpacked values compare?\n",
    "    print(ds[\"dh15\"].sel(lc=1, model=0, scenario=0, era=0, geom_id=12).values)\n",
    "    print(ds[\"dh15\"].sel(lc=1, model=0, scenario=0, era=1, geom_id=12).values)\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    print(file, \" size: \", (os.path.getsize(file) / (1024 * 1024 * 1024)), \"GB\")\n",
    "    # the xr.open_dataset() function should automatically apply the scaling and offset when we set mask_and_scale=True\n",
    "    ds_unpacked = xr.open_dataset(file, mask_and_scale=True)\n",
    "    test_unpacked_values(ds_unpacked)  # packed dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snap-geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
